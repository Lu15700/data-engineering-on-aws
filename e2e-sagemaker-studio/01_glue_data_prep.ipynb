{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18df7b03-92b3-4da7-a807-34b9941e120e",
   "metadata": {},
   "source": [
    "# Serverless Data Prep with Glue Interactive Sessions\n",
    "We can scale our data preparation using serverless Spark or Ray with native integration with AWS Glue Interactive Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeac588-afea-4e9f-970f-14194a1432bd",
   "metadata": {},
   "source": [
    "### What is AWS Glue\n",
    "\n",
    "AWS Glue is a serverless data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development.\n",
    "\n",
    "![img](https://d1.awsstatic.com/reInvent/reinvent-2022/glue/Product-Page-Diagram_AWS-Glue_for-Ray%402x.f34b47cf0280c7d843ea457b704ea512bebd91d5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182e44c-be7d-414b-beb8-1c101a3950ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Objective \n",
    "\n",
    "Want to predict the amount of NO2 in the area based on weather conditions\n",
    "\n",
    "![img](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Origins_of_acid_rain.svg/1280px-Origins_of_acid_rain.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbd41d-d861-4cd1-9261-4f5c62ec2e39",
   "metadata": {},
   "source": [
    "### Datasets in our Example\n",
    "\n",
    "[OpenAQ Physical Air Quality Data](https://registry.opendata.aws/openaq/):\n",
    "* Global, aggregated physical air quality data from public data sources provided by government, research-grade and other sources.\n",
    "* 42GB of Data\n",
    "\n",
    "\n",
    "[NOAA Global Surface Summary of Day](https://registry.opendata.aws/noaa-gsod/):\n",
    "* Global summary of day data for 18 surface meteorological elements are derived from the synoptic/hourly observations contained in USAF DATSAV3 Surface data and Federal Climate Complex Integrated Surface Hourly (ISH).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4cfd1-7fa2-4adf-886c-a723535d2bd4",
   "metadata": {},
   "source": [
    "### Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f95a45-57af-47d0-a599-076d9a5ba060",
   "metadata": {},
   "outputs": [],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df5863-fe16-4ad5-9afb-546ab55a0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%session_id_prefix air-analysis\n",
    "%glue_version 3.0\n",
    "%number_of_workers 10\n",
    "%idle_timeout 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4d2e55-39d2-4e88-9fd7-f114786de7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4226c44e-4223-4eed-92e1-c320adbd3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = <\"YOUR_S3_BUCKET\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e10f0-82f0-464a-b15a-40431ae5f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_df = spark.read.json(\"s3://openaq-fetches/realtime-gzipped/2022-01-05/1641340870.ndjson.gz\")\n",
    "df = spark.read.schema(schema_df.schema).json(\"s3://openaq-fetches/realtime-gzipped/20*\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38c59c-e708-445b-96c3-12ac13f6a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, lower, to_date\n",
    "\n",
    "yr_split_args = (df.date.utc, \"-\", 0)\n",
    "dfSea = df.filter(lower((df.city)).contains('seattle')).filter(df.parameter == \"no2\").withColumn(\"year\", split(*yr_split_args)[0]).cache()\n",
    "dfSea.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5805f8-a718-457a-92e2-faf79aea90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoAvg = dfSea.withColumn(\"ymd\", to_date(dfSea.date.utc)).groupBy(\"ymd\").avg(\"value\").withColumnRenamed(\"avg(value)\", \"no2_avg\")\n",
    "dfNoAvg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322044bb-6228-428c-a088-b694be8d95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to S3\n",
    "dfNoAvg.coalesce(1).write.parquet(f\"s3://{bucket}/subset-aggregate-no2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5f462-59f4-4883-a03e-ec996ccef4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, year\n",
    "year_min, year_max = dfNoAvg.select(year(min(\"ymd\")), year(max(\"ymd\"))).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2510a7d-1398-4aea-b66c-85d183064e5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a7efd-097a-4481-a2e3-85bec5e82def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Scope to Seattle, WA, USA\n",
    "longLeft, latBottom, longRight, latTop = [-122.459696,47.481002,-122.224433,47.734136]\n",
    "\n",
    "dfSchema = spark.read.csv(\"s3://noaa-gsod-pds/2022/32509099999.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# We read our first year, then union the rest of the years :)\n",
    "def read_year(year):\n",
    "    return spark.read.csv(f\"s3://noaa-gsod-pds/{year}/\", header=True, schema=dfSchema.schema)\n",
    "\n",
    "year_range = range(year_min, year_max+1)\n",
    "df = read_year(year_range[0])\n",
    "for year in year_range[1:]:\n",
    "    df = df.union(read_year(year))\n",
    "\n",
    "df = df \\\n",
    "        .withColumn('LATITUDE', df.LATITUDE.cast(DoubleType())) \\\n",
    "        .withColumn('LONGITUDE', df.LONGITUDE.cast(DoubleType()))\n",
    "\n",
    "seadf = df \\\n",
    "        .filter(df.LATITUDE >= latBottom) \\\n",
    "        .filter(df.LATITUDE <= latTop) \\\n",
    "        .filter(df.LONGITUDE >= longLeft) \\\n",
    "        .filter(df.LONGITUDE <= longRight)\n",
    "\n",
    "# Rename columns so they're easier to read\n",
    "seafeatures = seadf.selectExpr(\"Date as date\", \"MAX as temp_max\", \"MIN as temp_min\", \"WDSP as wind_avg\", \"SLP as pressure_sea_level\", \"STP as pressure_station\", \"VISIB as visibility\")\n",
    "\n",
    "# Remove invalid readings\n",
    "no_data_mappings = [\n",
    "    [\"temp_max\", 9999.9],\n",
    "    [\"temp_min\", 9999.9],\n",
    "    [\"wind_avg\", 999.9],\n",
    "    [\"pressure_sea_level\", 9999.9],\n",
    "    [\"pressure_station\", 9999.9],\n",
    "    [\"visibility\", 999.9],\n",
    "]\n",
    "for [name, val] in no_data_mappings:\n",
    "    seafeatures = seafeatures.withColumn(name, F.when(F.col(name)==val, None).otherwise(F.col(name)))\n",
    "    \n",
    "# Now average each reading per day\n",
    "seafeatures = seafeatures.groupBy(\"date\").agg(*[F.mean(c).alias(c) for c in seafeatures.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bda4fe-bf6a-42ae-bd0d-278f9b9e669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seafeatures.coalesce(1).write.parquet(f\"s3://{bucket}/subset-seattle-weather.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f21d5f-ae5f-47d9-b1ad-5c0099774f07",
   "metadata": {},
   "source": [
    "# End the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb0985-1454-4412-a543-7449c765f842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Glue PySpark (SparkAnalytics 2.0)",
   "language": "python",
   "name": "conda-env-sm_glue_is-glue_pyspark__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-sparkanalytics-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
